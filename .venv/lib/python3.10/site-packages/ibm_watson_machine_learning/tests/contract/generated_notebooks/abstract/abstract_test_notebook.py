#  -----------------------------------------------------------------------------------------
#  (C) Copyright IBM Corp. 2021- 2023.
#  https://opensource.org/licenses/BSD-3-Clause
#  -----------------------------------------------------------------------------------------

import abc
from os import environ

import unittest
import time

from sklearn.pipeline import Pipeline

from ibm_watson_machine_learning import APIClient
from ibm_watson_machine_learning.experiment import AutoAI
from ibm_watson_machine_learning.deployment import WebService, Batch
from ibm_watson_machine_learning.workspace import WorkSpace
from ibm_watson_machine_learning.helpers.connections import DataConnection
from ibm_watson_machine_learning.experiment.autoai.optimizers import RemoteAutoPipelines
from ibm_watson_machine_learning.tests.utils import (get_wml_credentials, get_cos_credentials, get_space_id,
                                                     is_cp4d)
from ibm_watson_machine_learning.tests.utils.cleanup import space_cleanup, delete_model_deployment
from ibm_watson_machine_learning.utils.autoai.enums import PredictionType, RunStateTypes

from ibm_watson_machine_learning.tests.utils.assertions import get_and_predict_all_pipelines_as_lale, \
    validate_autoai_experiment, validate_autoai_timeseries_experiment

from ibm_watson_machine_learning.utils.autoai.utils import chose_model_output


class AbstractTestNotebook(abc.ABC):
    """
    The abstract tests which covers:
    - training AutoAI model on a dataset
    - autogenerated notebook scenario

    In order to execute test connection definitions must be provided
    in inheriting classes.
    """

    bucket_name = environ.get('BUCKET_NAME', "wml-autoaitests-qa")
    pod_version = environ.get('KB_VERSION', None)
    space_name = environ.get('SPACE_NAME', 'regression_tests_sdk_space')

    cos_endpoint = "https://s3.us.cloud-object-storage.appdomain.cloud"
    results_cos_path = 'results_wml_autoai'

    required_experiment_metadata_fields = ['prediction_type', 'max_number_of_estimators', 'training_data_references',
                                           'training_result_reference']

    experiment_metadata_fields_to_be_updated_after_training = ['training_data_references', 'training_result_reference',
                                                               'holdout_size', 'scoring', 'csv_separator',
                                                               'max_number_of_estimators']

    # to be set in every child class:
    OPTIMIZER_NAME = "AutoAI regression test"

    SPACE_ONLY = True
    HISTORICAL_RUNS_CHECK = True

    experiment_info = dict(name=OPTIMIZER_NAME,
                           desc='test description',
                           prediction_type=PredictionType.MULTICLASS,
                           prediction_column='species',
                           autoai_pod_version=pod_version
                           )

    experiment_metadata: dict

    wml_client: 'APIClient' = None
    experiment: 'AutoAI' = None
    remote_auto_pipelines: 'RemoteAutoPipelines' = None
    wml_credentials = None
    cos_credentials = None
    pipeline_opt: 'RemoteAutoPipelines' = None
    service: 'WebService' = None
    service_batch: 'Batch' = None
    data_join_graph = None

    cos_resource_instance_id = None
    experiment_info: dict = None

    best_pipeline_name = False
    trained_pipeline_details = None
    run_id = None
    prev_run_id = None
    data_connection = None
    results_connection = None
    train_data = None

    pipeline: 'Pipeline' = None
    lale_pipeline = None
    deployed_pipeline = None
    hyperopt_pipelines = None
    new_pipeline = None
    new_sklearn_pipeline = None
    X_train = None
    y_train = None
    X_holdout = None
    y_holdout = None

    project_id = None
    space_id = None

    asset_id = None
    connection_id = None

    @classmethod
    def setUpClass(cls) -> None:
        """
        Load WML credentials from config.ini file based on ENV variable.
        """
        cls.wml_credentials = get_wml_credentials()
        cls.wml_client = APIClient(wml_credentials=cls.wml_credentials.copy())

        cls.cos_credentials = get_cos_credentials()
        cls.cos_endpoint = cls.cos_credentials.get('endpoint_url')
        cls.cos_resource_instance_id = cls.cos_credentials.get('resource_instance_id')

        cls.project_id = cls.wml_credentials.get('project_id')

    def test_00a_space_cleanup(self):
        space_checked = False
        while not space_checked:
            space_cleanup(self.wml_client,
                          get_space_id(self.wml_client, self.space_name,
                                       cos_resource_instance_id=self.cos_resource_instance_id),
                          days_old=7)
            space_id = get_space_id(self.wml_client, self.space_name,
                                    cos_resource_instance_id=self.cos_resource_instance_id)
            try:
                self.assertIsNotNone(space_id, msg="space_id is None")
                space_checked = True
            except AssertionError:
                space_checked = False

        AbstractTestNotebook.space_id = space_id

        if self.SPACE_ONLY:
            self.wml_client.set.default_space(self.space_id)
        else:
            self.wml_client.set.default_project(self.project_id)

    def test_01_initialize_AutoAI_experiment__pass_credentials__object_initialized(self):
        if self.SPACE_ONLY:
            AbstractTestNotebook.experiment_run_notebook = AutoAI(wml_credentials=self.wml_credentials.copy(),
                                                                  space_id=self.space_id)
        else:
            AbstractTestNotebook.experiment_run_notebook = AutoAI(wml_credentials=self.wml_credentials.copy(),
                                                                  project_id=self.project_id)

        self.assertIsInstance(self.experiment_run_notebook, AutoAI, msg="Experiment is not of type AutoAI.")

    @abc.abstractmethod
    def test_02_data_reference_setup(self):
        pass

    @abc.abstractmethod
    def test_02b_experiment_metadata_setup(self):
        pass

    def test_03_initialize_optimizer(self):

        if self.experiment_info.get('prediction_type') == PredictionType.FORECASTING:
            AbstractTestNotebook.experiment_info = validate_autoai_timeseries_experiment(self.experiment_info,
                                                                                         self.pod_version)
        else:
            AbstractTestNotebook.experiment_info = validate_autoai_experiment(self.experiment_info, self.pod_version,
                                                                              self.data_join_graph)

        AbstractTestNotebook.remote_auto_pipelines = self.experiment_run_notebook.optimizer(
            **AbstractTestNotebook.experiment_info)

        self.assertIsInstance(self.remote_auto_pipelines, RemoteAutoPipelines,
                              msg="experiment.optimizer did not return RemoteAutoPipelines object")

    def test_04_get_configuration_parameters_of_remote_auto_pipeline(self):
        parameters = self.remote_auto_pipelines.get_params()
        print(parameters)

        self.assertIsInstance(parameters, dict, msg='Config parameters are not a dictionary instance.')

    def test_05_fit_run_training_of_auto_ai_in_wml(self):
        AbstractTestNotebook.trained_pipeline_details = self.remote_auto_pipelines.fit(
            training_data_references=self.data_connections,
            training_results_reference=self.results_connection,
            background_mode=True)

        AbstractTestNotebook.run_id = self.trained_pipeline_details['metadata']['id']
        self.assertIsNotNone(self.data_connections[0].auto_pipeline_params,
                             msg='DataConnection auto_pipeline_params was not updated.')

    def test_06a_get_run_status(self):
        status = self.remote_auto_pipelines.get_run_status()
        self.assertNotEqual(status, RunStateTypes.COMPLETED)

    def test_06b_get_run_details(self):
        parameters = self.remote_auto_pipelines.get_run_details()
        print(parameters)
        self.assertIsNotNone(parameters)
        self.assertIn(self.run_id, str(parameters))

    def test_07_wait_for_first_pipeline(self):

        print(f"Run status = {self.remote_auto_pipelines.get_run_status()}")
        # note: check if first pipeline was generated

        is_ts = self.experiment_info.get('prediction_type') == PredictionType.FORECASTING

        metrics = self.wml_client.training.get_details(self.run_id)['entity']['status'].get('metrics', [])
        while chose_model_output("1", is_ts_metrics=is_ts) not in str(
                metrics) and self.remote_auto_pipelines.get_run_status() not in ['failed', 'canceled']:
            time.sleep(5)
            print(".", end=" ")
            metrics = self.wml_client.training.get_details(self.run_id)['entity']['status'].get('metrics', [])
        # end note

        status = self.remote_auto_pipelines.get_run_status()
        run_details = self.remote_auto_pipelines.get_run_details().get('entity')
        self.assertNotIn(status, ['failed', 'canceled'], msg=f"Training finished with status {status}. \n"
                                                             f"Details: {run_details.get('status')}")
        print("\n 1st pipeline completed")
        print(f"Run status = {status}")

    def test_08_update_experiment_metadata_data_referances(self):
        pipeline_parameters = self.remote_auto_pipelines.get_params()

        print(self.experiment_metadata)

        for param_to_add in self.experiment_metadata_fields_to_be_updated_after_training:
            if param_to_add == 'training_data_references':
                self.experiment_metadata[param_to_add] = self.remote_auto_pipelines.get_data_connections()
            elif param_to_add == 'test_data_references':
                self.experiment_metadata[param_to_add] = self.remote_auto_pipelines.get_test_data_connections()
            elif param_to_add == 'training_result_reference':
                results_reference = self.remote_auto_pipelines.get_run_details()['entity']['results_reference']
                result_data_conn = DataConnection._from_dict(results_reference)
                result_data_conn.location._model_location = results_reference['location'].get('path', '') + "/model.zip"
                self.experiment_metadata[param_to_add] = result_data_conn
            else:
                self.experiment_metadata[param_to_add] = pipeline_parameters.get(param_to_add)

    def test_09_validate_experiment_metadata(self):
        print(self.experiment_metadata)

        missing_params = []
        for r_parameter in self.required_experiment_metadata_fields:
            if r_parameter not in self.experiment_metadata:
                missing_params.append(r_parameter)

        self.assertEqual(len(missing_params), 0, f"Missing in experiment metadata: {missing_params}")

    ########
    # LALE #
    ########

    def test_31__get_pipeline__load_lale_pipeline__pipeline_loaded(self):
        AbstractTestNotebook.lale_pipeline = self.remote_auto_pipelines.get_pipeline()
        print(f"Fetched pipeline type: {type(self.lale_pipeline)}")

        from lale.operators import TrainablePipeline
        self.assertIsInstance(self.lale_pipeline, TrainablePipeline,
                              msg="Fetched pipeline is not of TrainablePipeline instance.")
        predictions = self.lale_pipeline.predict(
            X=self.X_values[:5])
        print(predictions)

    def test_32_get_all_pipelines_as_lale(self):
        get_and_predict_all_pipelines_as_lale(self.remote_auto_pipelines, self.X_values)
